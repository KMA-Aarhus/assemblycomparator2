# In the hpc-conda-slurm instance, this file is pointed to indirectly with --profile
# --profile /cluster/work/users/cmkobel/assemblycomparator2/configs/slurm/


# [--snakefile FILE] 
cores: 2048
# [--local-cores N] 
#configfile: "/home/cmkobel/assemblycomparator2/configs/workflow.yaml"
jobs: 200

# [--directory DIR] 
keep-going: true 
rerun-incomplete: true
keep-incomplete: true

# [--shadow-prefix DIR] 
# [--reason] 
# [--latency-wait SECONDS] 
# [--max-jobs-per-second MAX_JOBS_PER_SECOND] 
# [--greediness GREEDINESS] 
# [--runtime-profile FILE] 
show-failed-logs: true
#drmaa: "
#    --mem={cluster.mem}
#    --cpus-per-task={cluster.cpus-per-task} 
#    --time={cluster.time} 
#    --account={cluster.account}
#    --error={cluster.error} 
#    --output={cluster.output}
#"


cluster:
  mkdir -p logs/{rule} &&
  sbatch
    --parsable
    --cpus-per-task={threads}
    --mem-per-cpu={resources.mem_mb}
    --job-name=ac2_{rule}_{wildcards} 
    --output=logs/{rule}/{rule}-{wildcards}-%j.out
    --time={cluster.time}
    --account={cluster.account}


cluster-cancel: "scancel"
#FileNotFoundError: [Errno 2] No such file or directory: 'scancel %j'




#cluster:
#  mkdir -p logs/{rule} &&
#  sbatch
#    --partition={resources.partition}
#    --qos={resources.qos}
#    --cpus-per-task={threads}
#    --mem={resources.mem_mb}
#    --job-name=smk-{rule}-{wildcards}
#    --output=logs/{rule}/{rule}-{wildcards}-%j.out
#    --account=myaccount



# This should be given as a cli argument
#cluster-config: "/home/cmkobel/assemblycomparator2/configs/slurm/cluster.yaml" 


# [--jobscript SCRIPT] 
jobname: "{name}.{jobid}.snakejob.sh" 
#use-conda: true
#use-singularity: true
# [--conda-prefix DIR] 

latency-wait: 160 # This might seem excessive, but on GenomeDK increasing this value seemed to help with some tentatively failed jobs.


# TODO: clean up this file.
